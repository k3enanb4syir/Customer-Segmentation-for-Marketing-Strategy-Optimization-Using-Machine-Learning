{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13818675,"sourceType":"datasetVersion","datasetId":8799962}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/keenanbasyir/mall-customer-segmentation-using-machine-learning?scriptVersionId=283298446\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# **Customer Segmentation for Marketing Strategy Optimization**\n\n**Authors:** Muhammad Keenan Basyir, Fayyadh Dhia Abyan, Marvel Maliq Prabawa, Raihan Yudhistira Hartawan\n\n**Date:** November 2025  \n\n**Tech Stack:** Python, Scikit-Learn, Pandas, Seaborn  \n\n---\n\n## **ðŸ“‹ Project Overview**\n**The Problem:** The mall serves over 200 customers with distinct behaviors. However, the current marketing strategy relies on guesswork rather than data-driven insights. Manual analysis is inefficient, leading to missed opportunities for targeted engagement.\n\n**The Objective:**\n\n**Supervised Learning:** To propose a model that can classify future customers into these segments instantly. \n\n**Unsupervised Learning:** Automatically group customers into **distinct** segments based on their *Annual Income* and *Spending Score* to identify high-value targets.\n\n**Business Value:** Provide actionable recommendations for each segment to maximize revenue and customer retention.","metadata":{}},{"cell_type":"markdown","source":"## **ðŸš¦ Unsupervised Learning (K-Means Clustering)**","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.cluster import KMeans\nimport os\n\n# PART 1: LOAD DATA\n# Smart file finder\nfilename = 'Mall_Customers.csv'\nfile_path = None\nsearch_paths = [\n    '/kaggle/input/mall-customers/Mall_Customers.csv',\n    '/kaggle/input/mall-customer-segmentation-data/Mall_Customers.csv',\n    'Mall_Customers.csv'\n]\n\nfor path in search_paths:\n    if os.path.exists(path):\n        file_path = path\n        break\n\nif not file_path and os.path.exists('/kaggle/input'):\n    for root, dirs, files in os.walk('/kaggle/input'):\n        if filename in files:\n            file_path = os.path.join(root, filename)\n            break\n\nif file_path:\n    dataset = pd.read_csv(file_path)\n    print(f\"âœ… Data loaded from: {file_path}\")\nelse:\n    raise FileNotFoundError(\"CSV not found\")\n\n# Select Annual Income (Column 3) and Spending Score (Column 4)\nX = dataset.iloc[:, [3, 4]].values\n\n# PART 2: THE ELBOW METHOD\n\nprint(\"Elbow Method\")\nwcss = []\nfor i in range(1, 11):\n    # ADDED 'n_init=10' HERE TO FIX THE WARNING\n    kmeans = KMeans(n_clusters = i, init = 'k-means++', n_init=10, random_state = 42)\n    kmeans.fit(X)\n    wcss.append(kmeans.inertia_)\n\n# Plot the Elbow Graph\nplt.figure(figsize=(10, 5))\nplt.plot(range(1, 11), wcss, marker='o', color='blue')\nplt.title('The Elbow Method')\nplt.xlabel('Number of Clusters')\nplt.ylabel('WCSS')\nplt.grid(True)\nplt.show()\n\n# PART 3: K-MEANS CLUSTERING\n\nprint(\"Training K-Means with 5 Clusters\")\n# ADDED 'n_init=10' HERE TO FIX THE WARNING\nkmeans = KMeans(n_clusters = 5, init = 'k-means++', n_init=10, random_state = 42)\ny_kmeans = kmeans.fit_predict(X)\n\n# PART 4: VISUALIZE CLUSTERS\n\nplt.figure(figsize=(12, 8))\n\n# Plot each cluster\nplt.scatter(X[y_kmeans == 0, 0], X[y_kmeans == 0, 1], s = 100, c = 'red', label = 'Cluster 1 (Standard)')\nplt.scatter(X[y_kmeans == 1, 0], X[y_kmeans == 1, 1], s = 100, c = 'blue', label = 'Cluster 2 (Careless)')\nplt.scatter(X[y_kmeans == 2, 0], X[y_kmeans == 2, 1], s = 100, c = 'magenta', label = 'Cluster 5 (Budget)')\nplt.scatter(X[y_kmeans == 3, 0], X[y_kmeans == 3, 1], s = 100, c = 'cyan', label = 'Cluster 4 (Sensible)')\nplt.scatter(X[y_kmeans == 4, 0], X[y_kmeans == 4, 1], s = 100, c = 'green', label = 'Cluster 3 (Target/VIP)')\n\n# Plot Centroids\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], \n            s = 300, c = 'yellow', label = 'Centroids', edgecolors='black')\n\nplt.title('Customer Segments')\nplt.xlabel('Annual Income (k$)')\nplt.ylabel('Spending Score (1-100)')\nplt.legend()\nplt.grid(True)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T07:19:28.019819Z","iopub.execute_input":"2025-12-02T07:19:28.020114Z","iopub.status.idle":"2025-12-02T07:19:34.273155Z","shell.execute_reply.started":"2025-12-02T07:19:28.020086Z","shell.execute_reply":"2025-12-02T07:19:34.272231Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Explanation:**\n\nIn the first graph, the 'Elbow Method,' we looked for the point where the error rate flattens out. You can see the bend happens clearly at K=5, which tells us there are 5 natural groups of customers.\n\nIn the second graph, we visualized those 5 groups. For example, if you look at the Green cluster in the top right, those customers have high income and high spending scores. We identify them as our 'VIPs'. Conversely, the Blue cluster below them has high income but low spending, identifying them as 'Savers. ","metadata":{}},{"cell_type":"markdown","source":"# **ðŸŒ³ Supervised learning (Decision tree)**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import tree\nimport os\n\n# 1. LOAD DATA & PREPARE TARGETS\n# Smart file finder\nfilename = 'Mall_Customers.csv'\nfile_path = None\nsearch_paths = [\n    '/kaggle/input/mall-customers/Mall_Customers.csv',\n    '/kaggle/input/mall-customer-segmentation-data/Mall_Customers.csv',\n    'Mall_Customers.csv'\n]\n\nfor path in search_paths:\n    if os.path.exists(path):\n        file_path = path\n        break\n\nif not file_path and os.path.exists('/kaggle/input'):\n    for root, dirs, files in os.walk('/kaggle/input'):\n        if filename in files:\n            file_path = os.path.join(root, filename)\n            break\n\nif file_path:\n    dataset = pd.read_csv(file_path)\nelse:\n    raise FileNotFoundError(\"Could not find 'Mall_Customers.csv'\")\n\n# WE GENERATE THE \"CORRECT ANSWERS\" (CLUSTERS)\n# We use K-Means (K=5) to label every customer as Cluster 0, 1, 2, 3, or 4.\nX_clustering = dataset.iloc[:, [3, 4]].values # Income & Score\nkmeans = KMeans(n_clusters=5, init='k-means++', n_init=10, random_state=42)\ndataset['Cluster_Label'] = kmeans.fit_predict(X_clustering)\n\n\n# 2. PREPARE SUPERVISED DATA\n# INPUTS (X): We want to predict the cluster based on Gender, Age, and Income\n# (We hide 'Spending Score' because that's what we want to guess!)\n\n# Convert Gender to Numbers (Male=0, Female=1)\ndataset['Gender_Code'] = dataset['Gender'].map({'Male': 0, 'Female': 1})\n\n# Select Features: Age, Income, Gender_Code\nX = dataset[['Age', 'Annual Income (k$)', 'Gender_Code']]\ny = dataset['Cluster_Label']\n\n# Split into Training (80%) and Testing (20%)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# 3. TRAIN DECISION TREE\n# We limit depth to 4 to keep the tree simple and readable\nclf = DecisionTreeClassifier(criterion='entropy', max_depth=4, random_state=42)\nclf.fit(X_train, y_train)\n\n# Calculate Accuracy\naccuracy = clf.score(X_test, y_test) * 100\nprint(f\"Accuracy: {accuracy:.2f}%\")\n\n# 4. VISUALIZE THE TREE\nplt.figure(figsize=(20, 10))\nplot_tree(clf, \n          feature_names=['Age', 'Income', 'Gender'],  \n          class_names=['Cluster 0', 'Cluster 1', 'Cluster 2', 'Cluster 3', 'Cluster 4'],\n          filled=True, \n          rounded=True, \n          fontsize=10)\nplt.title(\"Decision Tree Logic for Customer Classification\")\nplt.show()\n\n# 5. TEST WITH A FAKE NEW CUSTOMER\n# Example: A 25-year-old Male earning $80k\nnew_customer = pd.DataFrame([[25, 80, 0]], columns=['Age', 'Annual Income (k$)', 'Gender_Code'])\nprediction = clf.predict(new_customer)\nprint(f\"Prediction for New Customer (Age 25, Income $80k): Cluster {prediction[0]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T07:19:34.274912Z","iopub.execute_input":"2025-12-02T07:19:34.275268Z","iopub.status.idle":"2025-12-02T07:19:35.105702Z","shell.execute_reply.started":"2025-12-02T07:19:34.275239Z","shell.execute_reply":"2025-12-02T07:19:35.10463Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**The Goal:** Now that we know there are 5 types of customers (from K-Means), we want to build a \"Robot\" that can guess which type a new person is.\n\n* **The Inputs:** The Robot is allowed to look at the customer's Age, Annual Income, and Gender.\n* **The Target:** The Robot must guess the correct Cluster Number (0-4).\n\n**The Training:**\n* We fed the Robot 80% of our data (160 customers).\n* It looked for patterns (e.g., \"Young people with low income usually belong to Cluster 2\").\n* It built a \"Tree\" of Yes/No questions to make decisions.\n\n**The Visualization:** The colorful tree graph shows the exact questions the model asks.\n* Example: The top box might ask: \"Is Income less than 60k?\"\nIf Yes, go Left. If No, go Right.\n\n**The Result:** We can now type in a new customer's details (like Age 25, Income $80k), and the model instantly tells us they are likely a VIP (Cluster 3).\n\nTo classify new customers without re-doing the entire analysis every time, we use a supervised learning method called a Decision Tree. Think of this like teaching the computer to play a game of \"20 Questions.\" We show the computer our existing customers, their age, income, gender, and which group they belong to and it automatically learns the rules that define each group. For example, it might figure out that \"anyone under 30 with a high income is usually in the VIP group.\" The result is a simple flowchart where we can drop in a new customer's details, follow the Yes/No paths based on their profile, and instantly predict exactly which customer segment they belong to.","metadata":{}},{"cell_type":"markdown","source":"## **Business Recommendations & Strategy**\n\nBased on the clustering analysis, we have identified 5 distinct customer personas:\n\n1.  **The Savers (Low Spend, High Income):**\n    * *Observation:* These customers earn well but spend very little.\n    * *Strategy:* Target with \"High-Yield Investment\" offers or value-based promotions to unlock their potential.\n\n2.  **The Generalists (Average Spend, Average Income):**\n    * *Observation:* The largest group with moderate behavior.\n    * *Strategy:* Retain with standard loyalty programs and periodic sales.\n\n3.  **The Targets / VIPs (High Spend, High Income):**\n    * *Observation:* The most valuable segment.\n    * *Strategy:* Assign dedicated relationship managers, offer exclusive access to new products, and VIP events.\n\n4.  **The Spendthrifts (High Spend, Low Income):**\n    * *Observation:* Young or impulsive buyers who spend beyond their means.\n    * *Strategy:* Recommended budget-friendly \"Buy Now, Pay Later\" options or discount coupons to maintain volume.\n\n5.  **The Conservatives (Low Spend, Low Income):**\n    * *Strategy:* Monitor churn but prioritize marketing spend on higher-yield segments.","metadata":{}}]}